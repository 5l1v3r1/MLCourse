{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# - Clean up the deprecated functions at the end of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PENALTY FUNCTIONS - SOME EXAMPLES\n",
    "\n",
    "# multiplier is a positive number > 0 that determines the slope\n",
    "\n",
    "# Linear Penalty Function\n",
    "def linearPenalty(x, multiplier=1): \n",
    "    return x * multiplier\n",
    "\n",
    "# Flipped/Inverse Linear Penalty Function\n",
    "def invLinearPenalty(x, multiplier=1):\n",
    "    return -x * multiplier\n",
    "\n",
    "# Linear for negative x and zero for positive x\n",
    "def leftLinearPenalty(x, multiplier=1):\n",
    "    if(x < 0): return -x * multiplier\n",
    "    else: return 0\n",
    "    \n",
    "# Linear for positive x and zero for negative x\n",
    "def rightLinearPenalty(x, multiplier=1):\n",
    "    if(x < 0): return 0\n",
    "    else: return x * multiplier\n",
    "\n",
    "# V shape penalty\n",
    "def VPenalty(x, multiplier=1):\n",
    "    if (x < 0): return -x * multiplier\n",
    "    else: return x\n",
    "    \n",
    "# Inverted V shape penalty\n",
    "def invertedVPenalty(x, multiplier=1):\n",
    "    if (x < 0): return x * multiplier\n",
    "    else: return -x * multiplier\n",
    "    \n",
    "# Positive parabola penalty\n",
    "def squaredPenalty(x, multiplier=1):\n",
    "    return (x**2) * multiplier\n",
    "\n",
    "# Inverted parabola penalty\n",
    "def invertedSquaredPenalty(x, multiplier=1):\n",
    "    return -(x**2) * multiplier\n",
    "\n",
    "# Non-linear penalty\n",
    "def nonLinearPenalty(x, multiplier=1):\n",
    "    return x + x**2 + x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "penaltyFunctions = {linearPenalty: \"Linear Penalty\", \n",
    "                    invLinearPenalty: \"Inverse Linear Penalty\",\n",
    "                    leftLinearPenalty: \"Left-Linear Penalty\",\n",
    "                    rightLinearPenalty: \"Right-Linear Penalty\",\n",
    "                    VPenalty: \"V Penalty\",\n",
    "                    invertedVPenalty: \"Inverted-V Penalty\",\n",
    "                    squaredPenalty: \"Squared Penalty\",\n",
    "                    invertedSquaredPenalty: \"Inverted Squared Penalty\",\n",
    "                    nonLinearPenalty: \"Non-Linear Penalty\"\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of error values, plot the penalty function\n",
    "# Error = Actual - Predicted value - This is always along a single dimension because the output is always a single\n",
    "# column in a dataset.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the penalty function for a given list of error values and a given penalty function\n",
    "def penaltyPlot(errorList, penaltyFunction):\n",
    "    # Set up the x-axis\n",
    "    num_points = 200\n",
    "    x = np.linspace(min(errorList), max(errorList), num_points)\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.set(xlabel='Predicted Value - Actual Value')\n",
    "    ax.set(ylabel='Penalty')\n",
    "    ax.axvline(x=0, color='black')\n",
    "    ax.axhline(y=0, color='black')\n",
    "    ax.set(title=penaltyFunctions[penaltyFunction])\n",
    "    ax.plot(x, list(map(penaltyFunction,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#penaltyPlot([1,2,3,4,5], squaredPenalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the packages to investigate the data\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a column of ones to the first column of a dataframe\n",
    "# and turn it into a matrix\n",
    "def df_addOnes(dataFrame):\n",
    "    vals = dataFrame.values\n",
    "    add_ones_column = zip(np.ones(len(dataFrame)), vals)\n",
    "    feature_matrix = np.matrix([val for val in add_ones_column])\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making it easy to calculate the total penalty over the entire dataset\n",
    "def penalty(df_features, df_output, paramater_value_list, penalty_function):\n",
    "    \n",
    "    # df_features is a dataframe of the features (no column of ones added)\n",
    "    # df_output is a dataframe of the output column (target variable)\n",
    "    # parameter_value_list is a list of w0, w1, ..., wn+1 where n is the number of features\n",
    "    #  i.e., the number of columns in df_features.\n",
    "    \n",
    "    # Cost of being wrong calculated over the entire data set\n",
    "    # Will take X and add a first column of 1s to it to enable the matrix multiplication\n",
    "    # Therefore: X is an m x n matrix and theta is a n x 1 matrix\n",
    "    \n",
    "    #### Turn the function inputs into matrices ####\n",
    "    # Get X and y into the right shapes for use in computeCost\n",
    "    # Add a first column of ones to the feature matrix\n",
    "    # Add a column of 1s to X \n",
    "    feature_matrix = df_addOnes(df_features)\n",
    "\n",
    "    output_matrix = np.matrix(df_output.values)\n",
    "    parameter_matrix = np.matrix(paramater_value_list).T\n",
    "    \n",
    "    #return feature_matrix.shape, parameter_matrix.shape, output_matrix.shape\n",
    "    \n",
    "    # Difference between the predicted and the actual value\n",
    "    error = (feature_matrix * parameter_matrix) - output_matrix\n",
    "    \n",
    "    # penaltyPerOutput is an m x 1 matrix where each element is the penalty for\n",
    "    # the input and its associated output for a particular value of W\n",
    "    \n",
    "    # Apply a penalty function to the errors from each row of the dataset\n",
    "    # Use the squared error penalty function\n",
    "    #penaltyPerOutput = np.power(error, 2)\n",
    "    penaltyPerOutput = list(map(penalty_function,error))\n",
    "    \n",
    "    # totalPenalty is the sum of the penalties of each row of the dataset\n",
    "    totalPenalty = np.sum(penaltyPerOutput)\n",
    "    \n",
    "    # The penalty of getting it wrong is 1/2m of the totalPenalty (normalized penalty)\n",
    "    # m is the number of rows in df_features\n",
    "    totalPenaltyNorm = totalPenalty / (2 * len(df_features))\n",
    "    \n",
    "    return totalPenaltyNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement Gradient Descent\n",
    "def gradientDescent(X, y, W, alpha, iters, penaltyFunction):\n",
    "    # NOTE: X is the original dataframe -- need to add a column of ones to it and make it a matrix\n",
    "    feature_matrix = df_addOnes(X)\n",
    "    # feature_matrix is a m x n matrix\n",
    "    # y is a m x 1 matrix\n",
    "    # W is a n x 1 matrix\n",
    "    \n",
    "    # Keep track of everything\n",
    "    sumError = np.zeros(shape=(len(W),1))\n",
    "    sumErrorNorm = np.zeros(shape=(len(W),1))\n",
    "    temp = np.matrix(np.zeros(W.shape))\n",
    "    cost = np.zeros(iters)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        # Calculate the non-normalized values for each W parameter\n",
    "        error = (feature_matrix * W) - y\n",
    "        # return feature_matrix.shape, W.shape, y.shape, error, feature_matrix[:,1]\n",
    "        \n",
    "        for j in range(len(W)):\n",
    "            # Multiply the error vector by the appropriate column of the feature_matrix and sum it\n",
    "            sumError[j] = np.sum(np.multiply(error, feature_matrix[:,j]))\n",
    "            \n",
    "            # Normalize the sumError using alpha and m\n",
    "            sumErrorNorm[j] = np.divide(np.multiply(sumError[j], alpha), len(feature_matrix))\n",
    "            \n",
    "            temp[j,0] = W[j,0] - sumErrorNorm[j]\n",
    "        \n",
    "        W = temp\n",
    "    \n",
    "        # Use the original dataframe in this call\n",
    "        # the penalty function likes to see W.T as input\n",
    "        cost[i] = penalty(X,y,W.T,penaltyFunction)\n",
    "            \n",
    "    # Return the value of W after iters iterations of gradient descent\n",
    "    # Return the penalty = cost for this W\n",
    "    return W, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DEPRECATED #####\n",
    "# Add a column of ones to the first column of a dataframe\n",
    "# and turn it into a matrix\n",
    "def df_addOnes_old(dataFrame):\n",
    "    vals = dataFrame.values\n",
    "    add_ones_column = zip(np.ones(len(dataFrame)), vals)\n",
    "    feature_matrix = np.matrix([val for val in add_ones_column])\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DEPRECATED ####\n",
    "# Making it easy to calculate the total penalty over the entire dataset\n",
    "def penalty_old(df_features, df_output, paramater_value_list):\n",
    "    \n",
    "    # df_features is a dataframe of the features (no column of ones added)\n",
    "    # df_output is a dataframe of the output column (target variable)\n",
    "    # parameter_value_list is a list of w0, w1, ..., wn+1 where n is the number of features\n",
    "    #  i.e., the number of columns in df_features.\n",
    "    \n",
    "    # Cost of being wrong calculated over the entire data set\n",
    "    # Will take X and add a first column of 1s to it to enable the matrix multiplication\n",
    "    # Therefore: X is an m x n matrix and theta is a n x 1 matrix\n",
    "    \n",
    "    #### Turn the function inputs into matrices ####\n",
    "    # Get X and y into the right shapes for use in computeCost\n",
    "    # Add a first column of ones to the feature matrix\n",
    "    # Add a column of 1s to X \n",
    "    feature_matrix = df_addOnes(df_features)\n",
    "\n",
    "    output_matrix = np.matrix(df_output.values)\n",
    "    parameter_matrix = np.matrix(paramater_value_list).T\n",
    "    \n",
    "    # Difference between the predicted and the actual value\n",
    "    # We're doing matrix multiplication and subtraction\n",
    "    error = (feature_matrix * parameter_matrix) - output_matrix\n",
    "    \n",
    "    # penaltyPerOutput is an m x 1 matrix where each element is the penalty for\n",
    "    # the input and its associated output for a particular value of W\n",
    "    \n",
    "    # Use the squared error penalty function\n",
    "    penaltyPerOutput = np.power(error, 2)\n",
    "    \n",
    "    # totalPenalty is the sum of the penalties of each row of the dataset\n",
    "    totalPenalty = np.sum(penaltyPerOutput)\n",
    "    \n",
    "    # The penalty of getting it wrong is 1/2m of the totalPenalty (normalized penalty)\n",
    "    # m is the number of rows in df_features\n",
    "    totalPenaltyNorm = totalPenalty / (2 * len(df_features))\n",
    "    \n",
    "    return totalPenaltyNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### DEPRECATED ####\n",
    "# Implement Gradient Descent\n",
    "def gradientDescent_old(X, y, W, alpha, iters):\n",
    "    # NOTE: X is the original dataframe -- need to add a column of ones to it and make it a matrix\n",
    "    feature_matrix = df_addOnes(X)\n",
    "    # feature_matrix is a m x n matrix\n",
    "    # y is a m x 1 matrix\n",
    "    # W is a n x 1 matrix\n",
    "    \n",
    "    # Keep track of everything\n",
    "    sumError = np.zeros(shape=(len(W),1))\n",
    "    sumErrorNorm = np.zeros(shape=(len(W),1))\n",
    "    temp = np.matrix(np.zeros(W.shape))\n",
    "    cost = np.zeros(iters)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        # Calculate the non-normalized values for each W parameter\n",
    "        error = (feature_matrix * W) - y\n",
    "        # return feature_matrix.shape, W.shape, y.shape, error, feature_matrix[:,1]\n",
    "        \n",
    "        for j in range(len(W)):\n",
    "            # Multiply the error vector by the appropriate column of the feature_matrix and sum it\n",
    "            sumError[j] = np.sum(np.multiply(error, feature_matrix[:,j]))\n",
    "            \n",
    "            # Normalize the sumError using alpha and m\n",
    "            sumErrorNorm[j] = np.divide(np.multiply(sumError[j], alpha), len(feature_matrix))\n",
    "            \n",
    "            temp[j,0] = W[j,0] - sumErrorNorm[j]\n",
    "        \n",
    "        W = temp\n",
    "    \n",
    "        # Use the original dataframe in this call\n",
    "        # the penalty function likes to see W.T as input\n",
    "        cost[i] = penalty(X,y,W.T,)\n",
    "            \n",
    "    # Return the value of W after iters iterations of gradient descent\n",
    "    # Return the penalty = cost for this W\n",
    "    return W, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### DEPRECATED ####\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate the data for a plot with slope = 1\n",
    "x, y = np.linspace(-500000,500000,500), np.linspace(-500000,500000,500)\n",
    "\n",
    "# Set up the plot area\n",
    "fig, ((ax11, ax12), \n",
    "      (ax21, ax22), \n",
    "      (ax31, ax32), \n",
    "      (ax41,ax42)) = \\\n",
    "plt.subplots(figsize=(15,15), nrows=4, ncols=2, sharey=False)\n",
    "\n",
    "# Generate the plots\n",
    "ax11.plot(x,y)\n",
    "ax12.plot(x,-y)\n",
    "\n",
    "def f21(x):\n",
    "    if(x < 0): return -x\n",
    "    else: return 0\n",
    "\n",
    "ax21.plot(x, list(map(f21,x)))\n",
    "\n",
    "def f22(x):\n",
    "    if(x < 0): return 0\n",
    "    else: return x\n",
    "ax22.plot(x, list(map(f22,x)))\n",
    "\n",
    "def f31(x):\n",
    "    if (x < 0): return -x\n",
    "    else: return x\n",
    "\n",
    "ax31.plot(x, list(map(f31,x)))\n",
    "\n",
    "def f32(x):\n",
    "    if (x < 0): return x\n",
    "    else: return -x\n",
    "\n",
    "ax32.plot(x, list(map(f32,x)))\n",
    "\n",
    "ax41.plot(x, y**2)\n",
    "\n",
    "ax42.plot(x, -y**2)\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle('A Variety of Penalty Functions', fontweight='bold')\n",
    "\n",
    "# Plot (1,1)\n",
    "ax11.set(xlabel='Predicted Value - Actual Value',\n",
    "         ylabel='Penalty'\n",
    "        )\n",
    "ax11.axvline(x=0, color='black')\n",
    "ax11.axhline(y=0, color='black')\n",
    "\n",
    "# Plot (1,2)\n",
    "ax12.set(xlabel='Predicted Value - Actual Value')\n",
    "ax12.axvline(x=0, color='black')\n",
    "ax12.axhline(y=0, color='black')\n",
    "\n",
    "# Plot (2,1)\n",
    "ax21.set(xlabel='Predicted Value - Actual Value', ylabel='Penalty')\n",
    "ax21.axvline(x=0, color='black')\n",
    "ax21.axhline(y=0, color='black')\n",
    "ax21.set_ylim([-500000,500000])\n",
    "\n",
    "# Plot (2,2)\n",
    "ax22.set(xlabel='Predicted Value - Actual Value')\n",
    "ax22.axvline(x=0, color='black')\n",
    "ax22.axhline(y=0, color='black')\n",
    "ax22.set_ylim([-500000,500000])\n",
    "\n",
    "# Plot (3,1)\n",
    "ax31.set(xlabel='Predicted Value - Actual Value')\n",
    "ax31.set(ylabel='Penalty')\n",
    "ax31.axvline(x=0, color='black')\n",
    "ax31.axhline(y=0, color='black')\n",
    "ax31.set_ylim([-500000,500000])\n",
    "\n",
    "# Plot (3,2)\n",
    "ax32.set(xlabel='Predicted Value - Actual Value')\n",
    "ax32.axvline(x=0, color='black')\n",
    "ax32.axhline(y=0, color='black')\n",
    "ax32.set_ylim([-500000,500000])\n",
    "\n",
    "# Plot (4,1)\n",
    "ax41.set(xlabel='Predicted Value - Actual Value')\n",
    "ax41.set(ylabel='Penalty')\n",
    "ax41.axvline(x=0, color='black')\n",
    "ax41.axhline(y=0, color='black')\n",
    "\n",
    "# Plot (4,2)\n",
    "ax42.set(xlabel='Predicted Value - Actual Value')\n",
    "ax42.axvline(x=0, color='black')\n",
    "ax42.axhline(y=0, color='black')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
